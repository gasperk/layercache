#summary Prefetching the data before it times out
#labels Featured

= Prefetching the data =

== The problem ==
When an item in the cache is too old, it gets removed. Likewise, if the cache is too small, some items are evicted prematurely, in order to make place for newer items. At that moment, all reads for such items will return null, and the data will be requested from the source (i.e. the database). The data will then be written to the caches, so it'll be available from there for subsequent reads.

But, if the number of requests for a certain item is high, several clients will detect a cache miss at the same time, and all of them will read the data from the source. For example, if there is 100 requests for an item per second, a few ten (up to a hundred in this example) identical queries will be issued to the database. This can put a lot of load on the database, especially if you consider that this could happen to several items at once.

Database's query cache can help you here, but not necessarily. MySQL cache, for example, has [http://www.mysqlperformanceblog.com/2006/07/27/mysql-query-cache/ a few problems].

So, how to avoid the database being slammed?

== The solution ==
The solution is simple and effective - prefetch. This means that an item will be prefetched _before_ its time is up, before it should be treated as stale. For prefetch, you can define two parameters: time and probability. Time defines how many seconds before an item is stale should prefetch be considered, and probability defines how probable is prefetch to happen.

To explain by example with an item that has TTL 360 (5 minutes), and with prefetch set to 60 seconds with 0.01 probability. When the item in the cache is within the last 60 seconds of its lifetime, each request will decide whether or not should it go fetch the item from the source. Probability 0.01 means 1 in 100, so this means only 1/100 of concurrent requests will go fetch the item, others will continue reading the item from the cache. When the item is written back by that 1/100 requests, its TTL will again be 360s, so no request will trigger prefetch for another 4 minutes.

To complicate things a bit: if an item is requested 10 times a second, this means that (if PHP's [http://www.php.net/mt_rand mt_rand()] behaves) in the first 10-second period of the last 60 seconds of 360-second lifetime of the item, only 1 request will fetch the item. All the others will just read from the cache as usual.

This results in a much less requests to the source, i.e. the database. Instead of _every_ request, only a few of them will fetch the item from the database. However, prefetch only solves the problem for items being "naturally evicted" (by being too old, considering TTL). Items that are prematurely evicted in order to make more space in the cache will still be slammed, because there is no way of detecting when a certain item will be evicted.

== Use only one prefetch per stack ==
If you decide to use [Prefetch prefetch], you may want to only use it on one cache in the stack - preferably the last (the last added via addCache, the first that is accessed). While prefetch works fine on any number of layers, it's the probability that renders it unusable on every cache below the first cache with prefetch. Because requests for an item are already reduced by the first prefetch, which reduces the number of reads on the lower caches, it's very probable that items won't be prefetched on lower caches, because not enough requests will be made. It would be the same as with only one prefetch - the last one added via _withPrefetch_.

== Choosing time and probability values ==
These values are important for performance. If sub-optimal values are used, this can result in either under-prefetching or over-prefetching.

An example will clear things up. Imagine a resource is requested 10 times per second. You put it in a cache with TTL=600 (10 minutes). Now let's see how prefetch works for the following values:
  * time=60, probability=0.5
  * time=60, probability=0.00001

== An example ==
To enable prefetch, you have to specify it for each cache you add to a stack.
{{{
LayerCache::
  forSource(new SomeDataSource)->
  addCache(new LayerCache_Cache_Memcache($memcache))->withTTL(360)->withPrefetch(60, 0.01)->
  addCache(new LayerCache_Cache_APC)->withTTL(60)->withPrefetch(10, 0.02)->
  toStack('data');
}}}